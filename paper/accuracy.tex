\section{Numerical Robustness}

Algorithms should compute the correct result. In the context of real quantities
and floating point numbers, true correctness is unattainable. We can only
seek to minimize the error. 

It is outside the scope of this paper to use rigourous frameworks such as those 
explained in \cite{Higham02, Jiang06}.
Nonetheless, we did not want to completely ignore numerical robustness, so we 
have made some implementation decisions based on heuristics.
made some design decisions based on heuristics.

\subsection{Error Analysis}

Floating numbers can store numbers of the form $\pm m \cdot 2^e$,
where we call $m$ the mantissa and $e$ the exponent. For double precision,
we have ranges $0 \leq m \leq 2^{53}$, $-1022 \leq e \leq 1023$.
When doing computations with floating point numbers, we have to deal with
errors. These can arise from rounding, but also imprecise measurements or
previous computations. We denote $\hat{x}$ for a computed value $x$, which
may not be equal. One way to measure this error would be to compute
$|\hat{x} - x|$. This is called the \textit{absolute error}. If $x$ is a
measurement of weight, we will get a factor $1000$ difference in error
depending on whether we use in grams or kilograms as units. For this reason,
we frequently look at the \textit{relative error} $\frac{|\hat{x} - x|}{|x|}$.
Equivalently, the relative error is $|\delta|$ where $\hat{x} = (1 + \delta)x$.

We use the following ubiquitous model for floating point operations. Let
$\oplus \in \{+, -, *, /\}$ be a primitive operation, and denote 
$fl(x \oplus y)$ for the result computed with floating point numbers. Then
$fl(x \oplus y) = (x \oplus y)(1 + \delta)$ for some $\delta$ smaller than
a number called the \textit{machine precision}. This is typically $2^{-53}$
for \texttt{double} and $2^{-24}$ for \texttt{float}. Though these operations
have the same small error, they magnify existing errors to a very different
extent.

Consider two perturbed values $\hat{a} = a(1 + \delta_a)$,
$\hat{b} = b(1 + \delta_b)$. Then for subtraction an attainable
upper bound on the relative error is

$$\frac{|a(1 + \delta_a) - b(1 + \delta_b) - (a - b)|}{|a - b|} = 
\frac{|\delta_a a - \delta_b b|}{|a - b|} \leq 
\frac{|\delta_a||a| + |\delta_b| |b|}{|a - b|} \leq 
\max(|\delta_a|, |\delta_b|) \frac{|a| + |b|}{|a - b|}.$$

So a small error in the input can lead to a large error in the result whenever 
$|a - b| \ll |a| + |b|$.

For multiplication we are in a much better position as we have an upper bound

$$\frac{|a(1 + \delta_a)b(1 + \delta_b) - ab|}{|ab|} =
\frac{|ab(\delta_a + \delta_b + \delta_a + \delta_b|}{|ab|} \leq
2\max(|\delta_a|, |\delta_b|) + \max(|\delta_a|, |\delta_b|)^2.$$

So the error is practically bounded by twice the machine precision.

\subsection{Accurate Evaluation of Geometric Tests}

As discussed, we can decide whether $puq$ makes a right-hand turn by testing
$orient(p, u, q) > 0$. We have $orient(p, u, q) = -orient(u, p, q)$, so
we can equivalently compare

$$(p_x - u_x) \cdot (q_y - p_y) - (p_y - u_y) \cdot (q_x - p_x) < 0 \iff$$
$$(p_x - u_x) \cdot (q_y - p_y) - (p_y - u_y) \cdot (q_x - p_x) < 0 \iff$$
$$(p_x - u_x) \cdot (q_y - p_y) < (p_y - u_y) \cdot (q_x - p_x).$$

The number of operations are the same, but we evaluate this in a loop over $u$, 
so this formulaton has the performance benefit that $q_x - p_x$ and $q_y - p_y$ 
can be lifted out of the loop.

We can test whether $u$ is further from $pq$ than $u'$ by comparing
$orient(u, p, q)$ to $orient(u', p, q)$. So from a performance standpoint it
is tempting to compute the orientation once for each point, and then do both
the right-turn test and the distance test with this quantity, but this
runs into precision problems. We are not very much concerned with the
subtraction of coordinates because they are input values and hence not
subject to errors (at least errors we control). However, the subtraction 
in the middle does have round-off errors we introduce by the multiplications.
This subtraction is ill-conditioned whenever

$$(p_x - u_x) \cdot (q_y - p_y) - (p_y - u_y) \cdot (q_x - p_x) \ll
|(p_x - u_x) \cdot (q_y - p_y)| + |(p_y - u_y) \cdot (q_x - p_x)|.$$

This can happen when for example $p = (-1, 1)$ and $u$, $q$ close to
the origin.

Note that this is not a problem for deciding whether $puq$ is a
right-hand turn or not because this concerns the sign of the orientation,
not its value. Both sides of the inequality
$(p_x - u_x) \cdot (q_y - p_y) < (p_y - u_y) \cdot (q_x - p_x)$ are
computed within twice the machine precision.

So instead of working with orientations directly, we use that

$$orient(p, u, q) > orient(p, u', q) \iff orient(u, p, q) < orient(u', p, q) 
\iff$$

$$(q_y - p_y) \cdot (u_x - u'_x) < (q_x - p_x) \cdot (u_y - u'_y).$$
