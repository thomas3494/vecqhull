\section{Numerical Robustness}

An algorithm should not only be efficient, but also correct. When dealing
with values in $\mathbb{R}$ it is unfeasible to compute exact solutions, as
almost no real numbers can be stored in finite memory. Instead we should
compute bounds on the approximations we do. The error between the true
solution $f(x)$ of a problem $x$, and the computed solution $\widehat{f(x)}$
is called the \textit{forward error}. 

We may also ask ourselves, for what $\hat{x}$ is our computed solution 
$\widehat{f(x)}$, the actual solution $f(\hat{x})$. We call the error between 
$x$ and $\hat{x}$ the \textit{backward error}. As the input has some 
uncertainty, either from previous calculations or physical
measurements, we may be satisfied with a small backward error as well.
After all, we have no way of knowing whether this was the problem we were 
actually interested in.

We can connect these two ideas together with the idea of 
\textit{perturbation analysis}:
how large is the error between $f(x)$ and $f(\hat{x})$ compared with
the error between $x$ and $\hat{x}$? If this is the same or small,
we call the problem \textit{well-conditioned}, if not \textit{ill-conditioned}. 
For a well-conditioned problem, as small backward error also implies a small 
forward error.

Algorithms whose forward and backward error grow slowly are called 
\textit{numerically stable}.

\subsection{Example: subtraction}

A classic example is subtraction. This will also be useful for our analysis.
The relative error $y$ of a number $x$ is the quantity $\frac{|x - y|}{x}$, or 
equivalently $|\delta|$ for $y = (1 + \delta)x$. The computed difference
$fl(a - b)$ of floating point numbers $a$, $b$ satisfies 
$fl(a - b) = (1 + \epsilon)(a - b)$, where $\epsilon$ is smaller than the
machine delta, $2^{-24}$ for single-precision and $2^{-53}$ for double 
precision. This means we have a forward error bounded by $\epsilon$, and by 
rewriting to $fl(a - b) = (1 + \epsilon)a - (1 + \epsilon)b$, also the backward 
error is bounded by $\epsilon$. To look at the conditioning, we subtract two 
perturbed values $\hat{a} = (1 + \delta_a)a$ and $\hat{b} = (1 + \delta_b)b$ 
and compute the relative error
$$\left|\frac{(a - b) - (\hat{a} - \hat{b})}{(a - b)}\right| = 
\frac{|\delta_a a + \delta_b b|}{|a - b|} \leq 
\max(|\delta_a|, |\delta_b|) \frac{|a| + |b|}{|a - b|}.$$

We see that subtraction is ill-conditioned when $a \approx b$: small errors 
$\delta_a$, $\delta_b$ in the input, are magnified by the subtraction. The 
reader may recognize this as catastrophic cancellation. Note that this is only
problematic if $a$, $b$ have been contaminated by error.

\subsection{Robustness Framework}

We first need to define what we consider errors. An intuitive distance between
sets is the Hausdorff distance $d(A, B) := \max_{a \in A}d(a, B)$. We do not
consider the error to grow larger when switching units from meters to 
centimeters, so we scale this distance by the maximum absolute value of $x$- and
$y$-coordinates $M$, yielding $d_M(A, B) := d(A, B) / M$. Once we fix $M$, this
is a valid distance. This distance has as property that if each point in $P$
is perturbed by at most $\delta M$ (Euclidean distance), the perturbed set 
$\tilde{P}$ satisfies $d(P, \tilde{P}) < \delta$.

We find that $d_M(CH(P), CH(\tilde{P})) \leq d_M(P, \tilde{P})$, so the convex 
hull problem is well-conditioned. \tkcomment{Use Euclidean distance underneath
Hausdorff.}

\subsection{Accurate Evaluation of Right-Hand Turn}

As discussed, we can decide whether $puq$ makes a right-hand turn by testing
$orient(p, u, q) > 0$. We can rewrite this to the inequality
$(p_x - u_x) \cdot (q_y - p_y) < (p_y - u_y) \cdot (q_x - p_x)$. 
We do not expect the subtractions to be dangerous as the arguments are input 
values, and hence not contaminated by error. Denoting the true evaluation of 
this inequality by $rt(p, u, q)$ and the computed evaluation by 
$\widehat{rt}(p, u, q)$, we quantify this in Lemma~\ref{lem:right-turn}. 
As $\widehat{rt}(p, u, q)$ is evaluated in a loop over $u$,
this formulation also has the performance benefit that 
$q_x - p_x$ and $q_y - p_y$ can be lifted out of the loop.

\begin{lemma}\label{lem:right-turn}
    If $\widehat{rt}(p, u, q)$ is true, but $rt(p, u, q)$ is not, or the
    other way around, then $d(u, pq) < \frac{6\epsilon}{1 - 6\epsilon}M$.
\end{lemma}

\begin{proof}
    We write $fl(\cdots)$ for the floating point evaluation of a primitive 
    operation. The IEEE-754 standard satisfies
    $fl(x \circ y) = (1 + \delta)(x \circ y)$, $|\delta| < \epsilon$ for
    $\circ \in \{+, -, \cdot\}$. This gives us
    $$fl(fl(p_x - u_x) fl(q_y - p_y)) < 
            fl(fl(p_y - u_y) fl(q_x - p_x)) \iff$$
    $$(p_x - u_x) (q_y - p_y) < \frac{(1 + \delta_1)(1 + \delta_2)(1 + \delta_3)}
    {(1 + \delta_4)(1 + \delta_5)(1 + \delta_6)} (p_y - u_y) (q_x - p_x).$$

    By a non-trivial (\tkcomment{but well-known?}) result, there is some $\theta$
    such that
    $$\frac{(1 + \delta_1)(1 + \delta_2)(1 + \delta_3)}{(1 + \delta_4)(1 + \delta_5)(1 + \delta_6)} = 1 + \theta$$
    and $|\theta| \leq \frac{6\epsilon}{1 - 6\epsilon} := \gamma_6$.
    That simplifies the equation to
    $$(p_x - u_x) (q_y - p_y) < (1 + \theta) (p_y - u_y) (q_x - p_x) \iff$$
    $$-\theta (p_y - u_y) (q_x - p_x) < orient(p, u, q).$$

    As $rt(p, u, q)$ is false, we have $orient(p, u, q) \leq 0$, so
    $|orient(p, u, q)| \leq \gamma_6 |(p_y - u_y) (q_x - p_x)|$.
    We can now compute upper bound
    $$d(u, pq) = \frac{orient(p, u, q)}{2 \lVert p - q \rVert} \leq$$
    $$\frac{\gamma_6 |(p_y - u_y) (q_x - p_x)|}{2 \sqrt{(q_x - p_x)^2 + (q_y - p_y)^2}} \leq$$
    $$\frac{1}{2}\gamma_6 |p_y - u_y| \leq 
            \frac{1}{2} \gamma_6 (|p_y| + |u_y|)\gamma_6 M = \gamma_6 M.$$

    The case $rt(p, u, q)$ true but $\hat{rt}(p, u, q)$ false is analogous.
\end{proof}

\subsection{Accurate Evaluation of Distance Test}

We can test whether $u$ is farther from $pq$ than $u'$ by comparing
$orient(u, p, q)$ to $orient(u', p, q)$. This makes it tempting
to compute the orientation once for each point, and then do both
the right-turn test and the distance test with this quantity. However, this
can run into precision problems because the middle subtraction in
$$(u_x - p_x)(q_y - u_y) \bm{-} (u_y - p_y)(q_x - u_x)$$
takes arguments that are poluted by error.
Instead we use the equivalent test
$$(q_y - p_y) (u_x - u'_x) < (q_x - p_x) (u_y - u'_y)$$
to eliminate any subtractions on computed values.
We write $frt(p, q, u, u')$ for the predicate $d(pq, u) > d(pq, u')$,
and $\widehat{frt}(p, q, u, u')$ for the computed predicate. This is also
accurate as proven in Lemma~\ref{lem:farther}.
\tkcomment{I guess this is not really distance, but more like signed-distance
as it can be negative if $u$ is to the other side of $pq$.}

\begin{lemma}\label{lem:farther}
    The computed predicate $\widehat{frt}$ is backward-stable in the 
    following sense. Let $M$ be an upper bound on the magnitude of 
    $x$- and $y$-coordinates of points in $P$. 
    Then if $\widehat{frt}(p, q, u, u')$ is true, there exists 
    $\hat{u}'$ such that

    $$\lVert u' - \hat{u}' \rVert < \frac{6\epsilon}{1 - 6\epsilon}M$$

    and $frt(p, q, \hat{u}, \hat{u}')$ is true. Furthermore, this 
    $\hat{u}'$ can be obtained by moving $u$ perpendicular to $pq$
    by a distance $\frac{6\epsilon}{1 - 6\epsilon}M$ or more.
\end{lemma}

\begin{proof}
    Suppose $\widehat{frt}(p, q, u, u')$ is true. Then analogously to 
    Lemma~\ref{lem:right-turn}, we have

    $$(q_y - p_y) (u_x - u'_x) - (q_x - p_x) (u_y - u'_y) < 
            \theta(q_x - p_x) (u_y - u'_y),$$

    for $|\theta| < \gamma_6$. If $frt(p, q, u, u')$ is false, then

    $$(q_y - p_y) (u_x - u'_x) - (q_x - p_x) (u_y - u'_y) \geq 0,$$

    so 

    $$|(q_y - p_y) (u_x - u'_x) - (q_x - p_x) (u_y - u'_y)| \leq 
        |\theta(q_x - p_x) (u_y - u'_y)|.$$

    We also have 

    $$(q_y - p_y) (u_x - u'_x) - (q_x - p_x) (u_y - u'_y) =
       orient(u, p, q) - orient(u', p, q) =$$
    $$2 d(u, pq) \lVert p - q \rVert - 2 d(u', pq) \lVert p - q \rVert = 
        2 \lVert p - q \rVert (d(u, pq) - d(u', pq)).$$

    Taking this together yields

    $$d(u, pq) - d(u', pq) \leq 
       |\theta|\frac{|(q_x - p_x) (u_y - u'_y)|}{2 \lVert p - q \rVert} \leq$$
    $$\frac{1}{2}|\theta| |u_y - u'_y| \leq |\theta|M.$$

    So we obtain $\hat{u}'$ by moving $u'$ perpendicular to $pq$ by at least
    $|\theta|M$.
\end{proof}

\subsection{Numerical Stability of Quickhull}

Like Quicksort, it is possible to construct an adverserial input where we have 
$n$ levels of recursion, yielding $O(n^2)$ runtime. Likewise, the forward
and backward error are also linear in the depth of the recursion.
The depth is usually $O(1)$ or $O(\log n)$, but can be $O(n)$ in the worst 
case (where the runtime also becomes unfeasible).

\begin{theorem}
    We have implemented Quickhull in a way that is numerically stable.
    The backward error is bounded by $\gamma_6$, and the forward error by
    $\gamma_6 d$, where $d$ is the depth of the recursion. 
\end{theorem}

\begin{proof}
    \tkcomment{We can view this algorithm as a single recursive call with 
    degenerate left-most point...}

    We start by showing inductively that \texttt{HULL} of 
    Algorithm~\ref{alg:quickhull_basic} computes a $\widehat{CH(\hat{P})}$
    with $d_M(\widehat{CH(\hat{P})}, CH(\hat{P})) < 2\gamma_6 d$ (forward error)
    and $d_M(\hat{P}, P) < 2\gamma_6 d$ (backward error), so long as $p$, $r$,
    $q$ are at most $\gamma_6 M$ away from $CH(P \cup \{p, r, q\})$.

    For both the base case and induction step, we perturb $P$ as follows.
    By Lemma~\ref{lem:right-turn}, all points that are classified incorrectly 
    have a distance of at most $\gamma_6 M$ to either $pr$ or $rq$. If 
    $u \in P$ has been incorrectly added $S_1$, we move it 
    $\frac{d(u, pr) + \gamma_6 M}{2}$ perpendicular to $pr$. This yields a 
    point $\hat{u}$ that satisfies $rt(p, u, r)$ and 
    $d(u, \hat{u}) < \gamma_6 M$ by Lemma~\ref{lem:right-turn}. We do the same 
    for points that have been incorrectly added to $S_2$. Points that were 
    incorrectly discarded are moved to $pr$ (or equivalently $rq$). By 
    Lemma~\ref{lem:farther} we move $r_1$ at most $\gamma_6 M$ further from
    $pr$ if it had been incorrectly classified as the farthest point, and 
    analogous for $r_2$. This is the same direction as we did for 
    Lemma~\ref{lem:right-turn}.
    As each point has been moved at most $\gamma_6 M$, this yields a perturbed 
    set $P'$ with subsets $S_1', S_2'$ such that $S_1'$, $S_2'$ are exact for $P'$, 
    and $d(P, P'), d(S_1, S_1'), d(S_2, S_2') < \gamma_6$.

    \textbf{Induction hypothesis:}
    So $CH(P' \cup \{p, r, q\}) = \{p\} \cup CH(S_1', p, r_1', r) \cup \{r\}
        CH(S_2', r, r_2', q) \cup \{q\}$.
    By induction hypothesis there exists $S_1''$ for which
    $d_M(\widehat{CH}(S_1'', p, r_1', r), CH(S_1', p, r_1', r)) < 
    (d - 1) \gamma_6$.
    By the perturbation analysis 
    $d_M(CH(S_1', p, r_1', r), CH(S_1, p, r_1, r)) \leq 
    d_M(S_1', S_1) \leq \gamma_6$.
    So by triangle inequality 
    $d_M(\widehat{CH}(S_1'', p, r_1', r), CH(S_1, p, r_1, r)) < d \gamma_6$.

    \textbf{Base case:}
    As there are no recursive calls, we have 
    $S_1 = S_2 = \emptyset$. The construction above gives us $P'$ for which
    $\{p, r, q\}$ is at most $\gamma_6$ from some set 
    $\{\hat{p}, \hat{r}, \hat{q}\}$ that are on $CH(P' \cup \{p, r, q\})$.
\end{proof}
